
#!/usr/bin/env python
# coding: utf-8

# In[ ]:


# ! pip install texthero





# pip install -U pip setuptools wheel
# ! python -m spacy download pl_core_news_sm

# ! pip install https://github.com/explosion/spacy-models/releases/download/pl_core_news_sm-3.0.0/pl_core_news_sm-3.0.0.tar.gz#egg=pl_core_news_sm==2.3.0
    
# ostatecznie dzialajace:
# ! pip install -U C:\\Users\\UN99LT\\Downloads\\pl_core_news_sm-3.0.0.tar.gz


# # setting and variables

# In[ ]:


# change code cells width
from IPython.display import display, HTML
display(
HTML(data = """
<style>
  div#notebook-container {width: 75%;}
  div#menubar-container {width: 65%;}
  div#maintoolbar-container {width: 55%;}
</style>

"""))



# display all outputs
from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = 'all'



# pandas DataFrame displayed output size
import pandas as pd
pd.options.display.max_rows = 500
pd.options.display.max_columns = 40

# turning off scientific format for number in pandas DataFrames
pd.set_option('display.float_format', lambda x: '%.3f' % x)

# size of plot made by plotnine
import plotnine
plotnine.options.figure_size = (20, 10) # width, hight

get_ipython().run_line_magic('run', '"modules/data_analysis_tools.py"')
get_ipython().run_line_magic('run', '"modules/text_mining_tools.py"')

import random
import spacy



# date varaibles (useful for quick filtering date variables)
var_date = ['data']

path_results = 'results'
data_folder_path = 'data'
data_file_name = 'Zbiór_uczący_full_bez_dubli.xlsx'
data_hdl_file_name = 'train_set.csv'
data_path = os.path.abspath(os.path.join(data_folder_path, data_file_name))
data_hdl_path = os.path.abspath(os.path.join(data_folder_path, data_hdl_file_name))


# # data preparing

# ## loading

# In[ ]:


data_full = pd.read_excel(io = data_path, sheet_name = 'Arkusz1')
data_kody = pd.read_excel(io = data_path, sheet_name = 'Podsumowanie+kody')
data_hdl  = data_hdl  = pd.read_csv(data_hdl_path, encoding='cp1250', sep = '\t')

data_full.shape
data_kody.shape
data_hdl.shape


# ## clearing variables names

# In[ ]:


data_full = data_full.rename(columns={'/' : 'data'
,'Nr referencyjny transakcji' : 'nr_transakcji'
,'Typ Transakcji.Typ transakcji (kod)' : 'typ_transakcji'
,'Typ transakcji na potrzeby KYC FCRM' : 'typ_transakcji_kyc_fcrm'
,'Nr IBAN uznanie' : 'nr_iban_uznanie'
,'Nr IBAN obciązenie' : 'nr_iban_uznanie'
,'Status transakcji' : 'status_transakcji'
,'Kwota uznania (wal)' : 'kwota_uznania_wal'
,'Kwota uznania (PLN)' : 'kwota_uznania_pl'
,'Kwota obciążenia (wal)' : 'kwota_obciazanie_wal'
,'Kwota obciążenia (PLN)' : 'kwota_obciazanie_pln'
,'Właściciel Rach Uzn.Nr kartoteki w ICBS' : 'wlasciciel_rach_uzn_ICBS'
,'Segment Właściciela Rach Uzn.Segment (kod)' : 'segment_wlasciciela_rach_uzn_segm'
,'Właściciel Rach Uzn.Nazwa skrócona' : 'wlasciciel_rach_uzn'
,'Właściciel Rach Obc.Nr kartoteki w ICBS' : 'wlasciciel_rach_obc_ICBS'
,'Segment Właściciela Rach Obc.Segment (kod)' : 'wlasciciela_rach_obc_segm'
,'Właściciel Rach Obc.Nazwa skrócona' : 'wlasciciel_rach_obc'
,'Tytuł płatnosci' : 'tytul_platnosci'
,'Tytuł płatnosci 1' : 'tytul_platnosci_1'
,'Tytuł płatnosci 2' : 'tytul_platnosci_2'
,'Tytuł płatnosci 3' : 'tytul_platnosci_3'
,'Tytuł płatnosci 4' : 'tytul_platnosci_4'
,'Dane benificjenta' : 'dane_benificjenta'
,'Dane benificjenta 1' : 'dane_benificjenta_1'
,'Dane benificjenta 2' : 'dane_benificjenta_2'
,'Dane benificjenta 3' : 'dane_benificjenta_3'
,'Dane benificjenta 4' : 'dane_benificjenta_4'
,'Dane zleceniodawcy' : 'dane_zleceniodawcy'
,'Dane zleceniodawcy 1' : 'dane_zleceniodawcy_1'
,'Dane zleceniodawcy 2' : 'dane_zleceniodawcy_2'
,'Dane zleceniodawcy 3' : 'dane_zleceniodawcy_3'
,'Dane zleceniodawcy 4' : 'dane_zleceniodawcy_4'
,'Kategoria' : 'kategoria'
,'wewnetrzne' : 'wewnetrzne'})


data_kody = data_kody.rename(columns={'Etykiety wierszy':'kategoria'
,'Reguła':'regula'})


data_kody = data_kody[['kategoria', 'regula']]



data_hdl = data_hdl.rename(columns={'transactionrefnr':'nr_transakcji'})


# ## merging data with categories

# In[ ]:


data_full = data_full.merge(data_kody , left_on='kategoria',right_on = 'kategoria', how = 'left')
data_full.shape


# ## mergin with data from hdl

# In[ ]:


hdl_vars = ['nr_transakcji', 'segm_code_db']

data_full = data_full.merge(data_hdl[hdl_vars] , left_on='nr_transakcji',right_on = 'nr_transakcji', how = 'left')
data_full.shape


# ## clearing variables categories

# In[ ]:


kategoria_dict_1 = {'Wpływy tytułami faktur/VAT/invoice':'faktura'
,'Wewnętrzne':'wewnetrzne'
,'INNE':'inne'
,'Wpłaty gotówkowe/utarg/wpłaty przez pracowników':'wplaty'
,'Rozliczenia z terminali płatniczych - SOA':'soa'
,'Wpływ od właściciela/udziałowca':'wplyw_wlasciciel_udzialowiec'
,'Wypłaty PayPro/payU':'wyplaty_paypro_payu'
,'Wynajem/dzierżawa/opłaty za media':'wynajem'
,'Faktoring':'factoring'
,'Kredyt':'kredyt'
,'Rezerwacja - biura podróży':'rezerwacja_biura_podrozy'
,'Cesja należności':'cesja_naleznosci'
,'Wpłaty w UP':'wsplaty_w_up'
,'refundacja NFZ':'refundacje_nfz'
,'Odsetki':'odsetki'
,'Zaliczki':'zaliczki'
,'Przewalutowania - Currency One':'currency_one'
,'Przewalutowania':'przewalutowania'
,'Opłata za ochronę':'ochrona'
,'Przesyłki kurierskie':'kurier'
,'Sądy i komornicy':'sad_komornicy'
,'Zwrot z podatku VAT/ Urząd Skarbowy':'zwrot_vat'
,'Polisy ubezpieczeniowe':'polisa_ubezp'
,'Zasilenie - z zewnętrznego konta/przeksięgowanie środków':'zew_konto_przeksiegowania'
,'Transfer odwrotny - DOLMA - SOD':'transfer_odwrotny'
,'Opłaty za nieruchomości - raty i ich zakup':'nieruchomosci'
,'Fundusze Inwestycyjne':'fundusze_inwestycyjne'
,'Zwrot płatności kartą':'zwrot_platnosci_karta'
,'Odszkodowanie':'odszkodowania'
,'Usługi księgowe':'ksiegowosc'
,'Dotacje z urzędów':'dotacje'
,'Przelew na telefon':'przelew_na_telefon'
,'Wynagrodzenie':'wynagrodzenie'
,'ZUS':'zus'
,'ZWIERCIADLANE KONTO PASYWNE':'zwierciadlane_konto_pasywne'
,'500+':'500_plus'
,'Transakcja korespondencka - NOSTRO':'nostro'
,'Zamknięcie rachunku':'zamkniecie_rachunku'}


# ## final preprocessed data

# In[ ]:


data_full = categorical_features_recoding(data=data_full, var='kategoria', dict_old_new=kategoria_dict_1, else_value = None, new_name = 'kategoria_1')
data = data_full[['data', 'kategoria','kategoria_1','tytul_platnosci', 'wewnetrzne', 'dane_benificjenta','dane_zleceniodawcy','nr_transakcji']]
data['nr_transakcji'] = data['nr_transakcji'].astype(str)
# split for train, test and valid set
data['set_type'] = random.choices([1,2,3], weights=[7,2,1], k = data.shape[0])


# saveo to excel
data.to_excel(os.path.abspath(os.path.join(data_folder_path, 'data_processed.xlsx')))


# # basic analysis

# ## liczebnosci

# In[30]:


data = pd.read_excel(os.path.abspath(os.path.join(data_folder_path, 'data_processed.xlsx')))
data['nr_transakcji'] = data['nr_transakcji'].astype(str)
data.shape
cross_tab(data=data, var_1 = 'kategoria_1', var_2 = None, round = 1, normalize = 'index')


# ## regexp

# In[32]:


# SCIAGA Z REGEX

# data['czy_faktura']=data['tytul_platosci'].str.extract(r'(^w{5})')

# S.str.count(r'(^F.*)').sum()

# S[S.str.match(r'(^P.*)')==True]

# string = 'zFV' 
# pattern = 
# bool(re.search(pattern, string=string, re.IGNORECASE


# import re
# str_ = 'a l a m a k o t a'
# print(re.sub('((?<=\s)(.*?)(?=\s))|(^(.*?)(?=\s))|((?<=\s)(.*?)$)', '_', str_))


# In[34]:


data = data.loc[data['wewnetrzne']==False]
data.loc[data.kategoria_1 == 'faktura', 'if_invoice'] = 1
data.loc[data.kategoria_1 != 'faktura', 'if_invoice'] = 0

data['if_invoice'] = data['if_invoice'].astype(int) 


i = 1
data['if_inv_regex_'+str(i)] = data['tytul_platnosci'].str.contains('[^\s]{1,}/[^\s]{1,}/[^\s]{1,}'); i+=i
data['if_inv_regex_'+str(i)] = data['tytul_platnosci'].str.contains('(^fv)|(\sfv)|(^fs)|(\sfs)|(^fu)|(\sfu)|(^fa)|(\sfa)|(F-VAT)|(f-a)|(f-ra)|(f-ry)|(vat)|(f\d)', case=False); i+=1
data['if_inv_regex_'+str(i)] = data['tytul_platnosci'].str.contains('(/inv/)|(invoice)', case=False); i+=1
data['if_inv_regex_'+str(i)] = data['tytul_platnosci'].str.contains('/sha', case=False); i+=1
data['if_inv_regex_'+str(i)] = data['tytul_platnosci'].str.contains('(^|\W)f(\W)', case=False); i+=1
data['if_inv_regex_'+str(i)] = data['tytul_platnosci'].str.contains('(^|\W)pro(\W)*(forma)*', case=False); i+=1
data['if_inv_regex_'+str(i)] = data['tytul_platnosci'].str.contains('(wp.ata)|(soa/)|(factor)|(faktor)|(delegacj)', case=False); i+=1



data['if_inv_regex_all'] = ((data['if_inv_regex_1']) | (data['if_inv_regex_2']) | (data['if_inv_regex_3']) | (data['if_inv_regex_4']) | (data['if_inv_regex_5']) | (data['if_inv_regex_6'])) & (~data['if_inv_regex_7'])

data['if_inv_regex_all'] = data['if_inv_regex_all'].astype(int)

t1 = cross_tab(data=data, var_1 = 'if_invoice', var_2 = 'if_inv_regex_all', round = 1, normalize = 'index')
t1


# In[32]:


pd.set_option('display.max_columns', None)
pd.set_option('display.expand_frame_repr', False)
pd.set_option('max_colwidth', -1)
pd.set_option('display.min_rows', 1000)

# wylistuj źle sklasyfikowane przez regex wiersze
data.loc[data['if_invoice']!=data['if_inv_regex_all'], ['tytul_platnosci', 'kategoria_1','if_invoice', 'if_inv_regex_all','nr_transakcji', 'dane_benificjenta', 'dane_zleceniodawcy']]

# t1 = data.loc[data['if_invoice']!=data['if_inv_regex_all'], ['tytul_platnosci', 'kategoria_1','if_invoice', 'if_inv_regex_all','nr_transakcji', 'dane_benificjenta', 'dane_zleceniodawcy']]
# t1["nr_transakcji"] = t1["nr_transakcji"].astype(str)
# t1 = t1[['tytul_platnosci', 'kategoria_1', 'if_inv_regex_all','nr_transakcji', 'dane_benificjenta', 'dane_zleceniodawcy']]


# t1 = t1.rename(columns={'if_inv_regex_all':'czy faktura wg reg ex (wyrażenia regularne) 1 - tak/ 0 -nie'})
# t1 = t1.drop(columns=['if_invoice'])

# # t1.to_csv('zle_sklasyfikowane.csv', sep = '\t')
# t1.to_csv('zle_sklasyfikowane.csv', encoding='cp1250', sep = '\t')


#  'utf8' # useful encodings: latin1, latin2, utf8, windows-1252


# In[69]:


y_var_raw = 'tytul_platnosci'
y_var = 'tytul_platnosci_processed'

polish_characters = 'ążźćńłóęś'

data[y_var] = data[y_var_raw].str.lower()
data[y_var] = data[y_var].str.replace('\s{2,}', ' ', regex = True) 
data[y_var] = data[y_var].str.replace('[a-z{a}]'.format(a=polish_characters), 'l', regex = True)                
data[y_var].replace('[0-9]','d', regex=True, inplace=True)
data[y_var].replace('[^ld\s/]','p', regex=True, inplace=True)

data[y_var + '_spaces'] = data[y_var]
data[y_var + '_spaces'] = data[y_var + '_spaces'].str.join(' ')

data['number_digits'] = data[y_var].str.count('d')
data['number_punct'] = data[y_var].str.count('p')
data['number_slash'] = data[y_var].str.count('/')
data['number_spaces'] = data[y_var].str.count(' ')
data['number_letters'] = data[y_var].str.count('l')
data['number_words'] = data['number_spaces'].add(1) 
data['number_chars'] = data[y_var].str.count('')
data['number_chars_no_spaces'] = data['number_chars'] - data['number_spaces']


data['number_digits_proc'] = data['number_digits']/data['number_chars']
data['number_punct_proc'] = data['number_punct']/data['number_chars']
data['number_slash_proc'] = data['number_slash']/data['number_chars']

data['avg_word_length']= data['number_chars_no_spaces']/data['number_words']
     
data['number_with_slash'] = data[y_var].str.contains('[^\s]{1,}/[^\s]{1,}/[^\s]{1,}', regex=True)


data[y_var + '_recode _1'] =   data[y_var_raw].str.replace('[^\s]{1,}/[^\s]{1,}/[^\s]{1,}','number_with_slash' , regex = True)


# ## lematisation

# In[70]:


# lematyzacja
nlp = spacy.load("pl_core_news_sm")
data[y_var + 'recode _1_' + 'lem']=data[y_var + '_recode _1'].apply(lambda row: " ".join([w.lemma_ for w in nlp(row)]))

data[y_var + 'recode _1_' + 'lem'].replace('[0-9]','0', regex=True, inplace=True)


data.to_excel(os.path.abspath(os.path.join(data_folder_path, 'data_processed_2.xlsx')))


# ## tf idf

# In[72]:


data = pd.read_excel(os.path.abspath(os.path.join(data_folder_path, 'data_processed_2.xlsx')))


df_vect = pd.DataFrame()

# from sklearn.feature_extraction.text import CountVectorizer
# count_vectorizer = CountVectorizer()
# # .fit_transfer TOKENIZES and COUNTS
# X = count_vectorizer.fit_transform(data[y_var + '_recode _1'])
# pd.DataFrame(X.toarray(), columns=count_vectorizer.get_feature_names())



# Initialize a vectorizer (wariant 1)
from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer(use_idf=True, max_features=20, stop_words=None, analyzer = 'char', ngram_range=(1, 5))
X = vectorizer.fit_transform(data[y_var + '_spaces'])
df_vect = pd.concat([df_vect, pd.DataFrame(X.toarray(), columns=[x + '_idf_1' for x in vectorizer.get_feature_names()])], axis = 1)




# Initialize a vectorizer (wariant 2)
from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer(use_idf=True, max_features=20, stop_words=None)
X = vectorizer.fit_transform(data[y_var + 'recode _1_' + 'lem'])
df_vect = pd.concat([df_vect, pd.DataFrame(X.toarray(), columns=[x + '_idf_2' for x in vectorizer.get_feature_names()])], axis = 1)


data = pd.concat([data, df_vect], axis = 1)
# encoding='utf-8'
# ngram_range=(1, 1)
# norm='l2'
# analyzer='word'

data.to_csv(os.path.abspath(os.path.join(data_folder_path, 'data_processed_3.csv')))


# # testy pakietów

# ## spacy

# In[2]:


import spacy


# In[3]:


nlp = spacy.load("pl_core_news_sm")


# In[26]:


docs = list(nlp.pipe(data.tytul_platnosci))


# In[28]:


def extract_tokens_plus_meta(doc:spacy.tokens.doc.Doc):
    """Extract tokens and metadata from individual spaCy doc."""
    return [
        (i.text, i.i, i.lemma_, i.ent_type_, i.tag_, 
         i.dep_, i.pos_, i.is_stop, i.is_alpha, 
         i.is_digit, i.is_punct) for i in doc
    ]

def tidy_tokens(docs):
    """Extract tokens and metadata from list of spaCy docs."""
    
    cols = [
        "doc_id", "token", "token_order", "lemma", 
        "ent_type", "tag", "dep", "pos", "is_stop", 
        "is_alpha", "is_digit", "is_punct"
    ]
    
    meta_df = []
    for ix, doc in enumerate(docs):
        meta = extract_tokens_plus_meta(doc)
        meta = pd.DataFrame(meta)
        meta.columns = cols[1:]
        meta = meta.assign(doc_id = ix).loc[:, cols]
        meta_df.append(meta)
        
    return pd.concat(meta_df)  


# In[29]:


data_docs_tidy = tidy_tokens(docs)


# In[35]:


data_docs_tidy


# In[36]:


data_docs_tidy.groupby("doc_id").size().hist(figsize=(14, 7), color="red", alpha=.4, bins=50);


# In[37]:


data_docs_tidy.query("ent_type != ''").ent_type.value_counts()


# In[ ]:





# In[39]:


data_docs_tidy.query("is_stop == False & is_punct == False").lemma.value_counts().head(30).plot(kind="barh", figsize=(24, 14), alpha=.7)
plt.yticks(fontsize=20)
plt.xticks(fontsize=20);



# In[38]:



import spacy
nlp = spacy.load("pl_core_news_sm")

doc = nlp(u"Apples and oranges are similar. Boots and hippos aren't.")## lemmatization



# doc.


# In[15]:


import spacy
nlp = spacy.load("pl_core_news_sm")

doc = nlp(u"Apples and oranges are similar. Boots and hippos aren't.")

" ".join([token.lemma_ for token in doc])

lista = list()
for token in doc:
    lista.append(token.lemma_)
    print(token.lemma_)


# ### lematization

# In[22]:


import pandas as pd
data_test = pd.DataFrame({'a':['ala na kota','kot ma hiv']}) 


data_test['b']=data_test["a"].apply(lambda row: " ".join([w.lemma_ for w in nlp(row)]))


# ## vectorisation

# ### td-idf

# In[42]:


from sklearn.feature_extraction.text import CountVectorizer
count_vectorizer = CountVectorizer()
# .fit_transfer TOKENIZES and COUNTS
X = count_vectorizer.fit_transform(data.tytul_platnosci)



# sklearn.feature_extraction.text.TfidfVectorizer(*, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, analyzer='word', stop_words=None, token_pattern='(?u)\b\w\w+\b', ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.float64'>, norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)[source]¶


# In[44]:



pd.DataFrame(X.toarray(), columns=count_vectorizer.get_feature_names())


# In[47]:


# Initialize a vectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer(use_idf=True, max_features=10)
X = vectorizer.fit_transform(data.tytul_platnosci)


encoding='utf-8'
ngram_range=(1, 1)
norm='l2'
analyzer='word'


# In[ ]:





# # Classification

# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:




